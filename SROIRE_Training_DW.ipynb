{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dawido69929/SI_Projekt_Zaliczeniowy/blob/main/SROIRE_Training_DW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKkDhv2To0IB"
      },
      "source": [
        "# Full SROIE Text Recognition Pipeline: Preprocessing, Training, Testing, and Saving Model"
      ],
      "id": "SKkDhv2To0IB"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1DptE_lIczXj",
        "outputId": "1cfddc3c-1d3d-415c-9f55-6780cef87d1c"
      },
      "id": "1DptE_lIczXj",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets"
                ]
              },
              "id": "176e185260e24ae48300f88eb69970e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fqXi9k9o0IF"
      },
      "source": [
        "## 1. Setup and Imports"
      ],
      "id": "9fqXi9k9o0IF"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1qeU17Do0IG",
        "outputId": "d1d866fd-f0b6-4b76-d374-5aa55c8e41c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "\n",
        "import os\n",
        "# Attempt to disable XLA auto-JIT more broadly\n",
        "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=0'\n",
        "# Also try to disable JIT compilation at the TF config level\n",
        "import tensorflow as tf\n",
        "#try:\n",
        "#    tf.config.optimizer.set_jit(False)\n",
        "#    print(\"Attempted to tf.config.optimizer.set_jit(False)\")\n",
        "#except AttributeError:\n",
        "#    print(\"tf.config.optimizer.set_jit not available in this TF version (might be TF1.x specific or changed path). Continuing...\")\n",
        "\n",
        "\n",
        "#tf.debugging.enable_check_numerics()\n",
        "#print(\"TF Global Numerics Check ENABLED\")\n",
        "#print(\"Attempted to disable XLA Auto JIT globally.\")\n",
        "\n",
        "# ... rest of your imports (numpy, matplotlib, keras, datasets, etc.)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "# print(f\"Keras version: {keras.__version__}\") # If you're using standalone Keras"
      ],
      "id": "a1qeU17Do0IG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBXe14Wlo0IH"
      },
      "source": [
        "## 1.1 Mount Google Drive (if using Google Colab)"
      ],
      "id": "NBXe14Wlo0IH"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Nv0ABfo0IH",
        "outputId": "ac9f1121-9ac7-4d84-f8e9-d256cf4f80a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    GOOGLE_DRIVE_MOUNTED = True\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    GOOGLE_DRIVE_MOUNTED = False\n",
        "    print(\"Not running in Google Colab or Google Drive could not be mounted.\")"
      ],
      "id": "95Nv0ABfo0IH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3z8zlzvo0II"
      },
      "source": [
        "## 2. Configuration"
      ],
      "id": "d3z8zlzvo0II"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsQ02HYEo0IJ",
        "outputId": "4c3074c5-411d-441a-c7ee-e05a897a76c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model save directory set to: /content/drive/MyDrive/Colab Notebooks/Models\n",
            "Model will be saved to: /content/drive/MyDrive/Colab Notebooks/Models/sroie_crnn_model.keras\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Configuration\n",
        "\n",
        "# --- Configuration for Image Preprocessing ---\n",
        "TARGET_PREPROC_HEIGHT = 64\n",
        "\n",
        "# MODEL_DOWNAMPLE_FACTOR will be 2 if using the extremely simplified CNN in Cell 7\n",
        "MODEL_DOWNAMPLE_FACTOR = 2\n",
        "# MAX_MODEL_WIDTH needs to ensure logit_length >= 68\n",
        "# logit_length = MAX_MODEL_WIDTH // MODEL_DOWNAMPLE_FACTOR\n",
        "# MAX_MODEL_WIDTH // 2 >= 68  => MAX_MODEL_WIDTH >= 136\n",
        "MAX_MODEL_WIDTH = 144 # Gives logit_length = 144/2 = 72\n",
        "\n",
        "# --- Configuration for Model and Training ---\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10 # Keep low for testing\n",
        "\n",
        "# --- Configuration for Saving Model ---\n",
        "MODEL_SAVE_NAME = \"sroie_crnn_model.keras\"\n",
        "\n",
        "if GOOGLE_DRIVE_MOUNTED:\n",
        "    base_drive_path = '/content/drive/MyDrive/'\n",
        "    relative_model_path = 'Colab Notebooks/Models'\n",
        "    DRIVE_MODEL_SAVE_DIR = os.path.join(base_drive_path, relative_model_path)\n",
        "    DRIVE_MODEL_SAVE_PATH = os.path.join(DRIVE_MODEL_SAVE_DIR, MODEL_SAVE_NAME)\n",
        "    print(f\"Model save directory set to: {DRIVE_MODEL_SAVE_DIR}\")\n",
        "    print(f\"Model will be saved to: {DRIVE_MODEL_SAVE_PATH}\")\n",
        "else:\n",
        "    DRIVE_MODEL_SAVE_DIR = '.'\n",
        "    DRIVE_MODEL_SAVE_PATH = MODEL_SAVE_NAME\n",
        "    print(f\"Model will be saved locally to: {DRIVE_MODEL_SAVE_PATH}\")"
      ],
      "id": "YsQ02HYEo0IJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0MgUKOJo0IK"
      },
      "source": [
        "## 3. Load and Split Dataset"
      ],
      "id": "L0MgUKOJo0IK"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXBNMw7mo0IK",
        "outputId": "8d682d46-00a1-4a96-e622-4b10ad531286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG MODE: Using only 25.0% of the training data: 8406 samples.\n",
            "Full original SROIE train dataset size: 33626\n",
            "Effective full train dataset size (after potential subsetting): 8406\n",
            "Using final train dataset size: 7566\n",
            "Validation dataset size: 840\n",
            "Test dataset size: 18704\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Load and Split Dataset\n",
        "\n",
        "# Load datasets\n",
        "train_dataset_hf_full = load_dataset(\"priyank-m/SROIE_2019_text_recognition\", split=\"train\")\n",
        "test_dataset_hf  = load_dataset(\"priyank-m/SROIE_2019_text_recognition\", split=\"test\")\n",
        "\n",
        "# --- FOR FASTER DEBUGGING: Use a smaller subset ---\n",
        "# To use a subset (e.g., 5% of data for ~94 steps per epoch if BATCH_SIZE=16 for original full train)\n",
        "subset_percentage = 0.25\n",
        "# Comment out the next line to use the full training dataset\n",
        "train_dataset_hf_full = train_dataset_hf_full.select(range(int(len(train_dataset_hf_full) * subset_percentage)))\n",
        "if 'subset_percentage' in locals() and subset_percentage < 1.0:\n",
        "    print(f\"DEBUG MODE: Using only {subset_percentage*100:.1f}% of the training data: {len(train_dataset_hf_full)} samples.\")\n",
        "# --- END DEBUG SUBSET ---\n",
        "\n",
        "\n",
        "validation_dataset_size = int(len(train_dataset_hf_full) * 0.1) # 10% of the (potentially subsetted) training data\n",
        "\n",
        "# Ensure validation_dataset_size and remaining training data are at least 1, handle small dataset cases\n",
        "if len(train_dataset_hf_full) == 0:\n",
        "    raise ValueError(\"Training dataset is empty after subsetting or loading.\")\n",
        "if validation_dataset_size == 0 and len(train_dataset_hf_full) > 0:\n",
        "    validation_dataset_size = 1 # At least one sample for validation if possible\n",
        "if validation_dataset_size >= len(train_dataset_hf_full): # if val_size would leave no training data\n",
        "    validation_dataset_size = len(train_dataset_hf_full) // 2 # or some other fraction\n",
        "    if validation_dataset_size == 0 and len(train_dataset_hf_full) > 0 : validation_dataset_size = 1\n",
        "\n",
        "\n",
        "validation_dataset_hf = train_dataset_hf_full.select(range(validation_dataset_size))\n",
        "train_dataset_hf = train_dataset_hf_full.select(range(validation_dataset_size, len(train_dataset_hf_full)))\n",
        "\n",
        "# Final check if train_dataset_hf became empty after split\n",
        "if len(train_dataset_hf) == 0 and len(train_dataset_hf_full) > 0:\n",
        "    print(\"Warning: train_dataset_hf is empty after split, using a small part of original for training.\")\n",
        "    # This can happen if subsetting makes train_dataset_hf_full too small\n",
        "    # Re-adjust: make train at least 1, val at least 1 if possible\n",
        "    if len(train_dataset_hf_full) > 1:\n",
        "        train_dataset_hf = train_dataset_hf_full.select(range(1)) # train gets 1 sample\n",
        "        validation_dataset_hf = train_dataset_hf_full.select(range(1,len(train_dataset_hf_full))) # val gets the rest\n",
        "        if len(validation_dataset_hf) == 0: # if only 1 sample total\n",
        "             validation_dataset_hf = train_dataset_hf # use same for val\n",
        "    elif len(train_dataset_hf_full) == 1:\n",
        "        train_dataset_hf = train_dataset_hf_full\n",
        "        validation_dataset_hf = train_dataset_hf_full\n",
        "\n",
        "\n",
        "print(f\"Full original SROIE train dataset size: {len(load_dataset('priyank-m/SROIE_2019_text_recognition', split='train'))}\")\n",
        "print(f\"Effective full train dataset size (after potential subsetting): {len(train_dataset_hf_full)}\")\n",
        "print(f\"Using final train dataset size: {len(train_dataset_hf)}\")\n",
        "print(f\"Validation dataset size: {len(validation_dataset_hf)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset_hf)}\")"
      ],
      "id": "tXBNMw7mo0IK"
    },
    {
      "cell_type": "code",
      "source": [
        "longest_true_label_len = 0\n",
        "for text_sample in load_dataset(\"priyank-m/SROIE_2019_text_recognition\", split=\"train\")['text']: # Load fresh full dataset text\n",
        "    if len(str(text_sample)) > longest_true_label_len:\n",
        "        longest_true_label_len = len(str(text_sample))\n",
        "print(f\"LONGEST ACTUAL LABEL LENGTH IN FULL TRAINING DATASET: {longest_true_label_len}\")\n",
        "calculated_logit_length = MAX_MODEL_WIDTH // MODEL_DOWNAMPLE_FACTOR\n",
        "print(f\"CALCULATED LOGIT LENGTH (MODEL OUTPUT SEQUENCE LENGTH): {calculated_logit_length}\")\n",
        "if longest_true_label_len > calculated_logit_length:\n",
        "    print(f\"CRITICAL ERROR: Longest true label ({longest_true_label_len}) is longer than model output sequence length ({calculated_logit_length}). CTC will fail.\")\n",
        "else:\n",
        "    print(\"Label length vs. logit length constraint seems OK.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdlYBwgox0MA",
        "outputId": "c3180133-e5c0-4701-9653-8b20974fd89d"
      },
      "id": "MdlYBwgox0MA",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LONGEST ACTUAL LABEL LENGTH IN FULL TRAINING DATASET: 68\n",
            "CALCULATED LOGIT LENGTH (MODEL OUTPUT SEQUENCE LENGTH): 72\n",
            "Label length vs. logit length constraint seems OK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Z6QOECo0IM"
      },
      "source": [
        "## 4. Character Set and Label Encoding"
      ],
      "id": "83Z6QOECo0IM"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySvv9kdMo0IM",
        "outputId": "79b6c10c-1b0a-4ada-c858-33d005376cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 64 unique characters:  !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_|\n",
            "Vocabulary size (including CTC blank): 65\n",
            "Using MAX_LABEL_LENGTH (for y_true padding): 68\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Character Set and Label Encoding\n",
        "\n",
        "# Use the text from the full training dataset to build a robust vocabulary\n",
        "# If you used subsetting in Cell 3 for train_dataset_hf_full, this will use that subset.\n",
        "# For a truly complete vocabulary, load the 'text' column from the original full dataset again if needed.\n",
        "# For now, assume train_dataset_hf_full (potentially subsetted) is sufficient for vocab.\n",
        "all_texts_for_vocab = train_dataset_hf_full['text']\n",
        "\n",
        "characters = set()\n",
        "for text in all_texts_for_vocab:\n",
        "    characters.update(list(str(text))) # Ensure text is string\n",
        "characters = sorted(list(characters))\n",
        "\n",
        "print(f\"Found {len(characters)} unique characters: {''.join(characters)}\")\n",
        "\n",
        "char_to_num = {char: i + 1 for i, char in enumerate(characters)} # 0 is reserved for CTC blank\n",
        "num_to_char = {i + 1: char for i, char in enumerate(characters)}\n",
        "CTC_BLANK_INDEX = 0\n",
        "num_to_char[CTC_BLANK_INDEX] = \"<BLANK>\"\n",
        "\n",
        "VOCAB_SIZE = len(characters) + 1\n",
        "print(f\"Vocabulary size (including CTC blank): {VOCAB_SIZE}\")\n",
        "\n",
        "# Determine max label length for padding y_true.\n",
        "# This should be based on the longest actual label string in the *entire dataset* (e.g., 68).\n",
        "# The current all_texts_for_vocab might be from a subset.\n",
        "# For robustness, let's re-fetch all texts just for this calculation if not too slow.\n",
        "# If load_dataset is slow, you can manually set MAX_LABEL_LENGTH = 68 if you are certain.\n",
        "# temp_all_texts_for_max_len = load_dataset(\"priyank-m/SROIE_2019_text_recognition\", split=\"train\")['text']\n",
        "# MAX_LABEL_LENGTH = 0\n",
        "# for text in temp_all_texts_for_max_len:\n",
        "#     if len(str(text)) > MAX_LABEL_LENGTH:\n",
        "#         MAX_LABEL_LENGTH = len(str(text))\n",
        "# print(f\"Max label length from full dataset (for y_true padding): {MAX_LABEL_LENGTH}\")\n",
        "\n",
        "# Simpler: use the max from the current all_texts_for_vocab, assuming it's representative\n",
        "# Or if you know the true max is 68, you can hardcode it for safety:\n",
        "MAX_LABEL_LENGTH = 68\n",
        "print(f\"Using MAX_LABEL_LENGTH (for y_true padding): {MAX_LABEL_LENGTH}\")\n",
        "# This ensures y_true arrays are padded to at least 68.\n",
        "# The actual encoded length of a label can be shorter."
      ],
      "id": "ySvv9kdMo0IM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIFGIRTxo0IN"
      },
      "source": [
        "## 5. Image and Label Preprocessing Function"
      ],
      "id": "aIFGIRTxo0IN"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vl9p-1igo0IN"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Image and Label Preprocessing Function\n",
        "\n",
        "def encode_text_to_labels(text_tensor, char_map_py, max_len_py_for_padding):\n",
        "    # text_tensor: the string tensor for a single label\n",
        "    # char_map_py: Python dictionary char -> num\n",
        "    # max_len_py_for_padding: integer, length to pad the output encoded label array to (e.g., dataset's MAX_LABEL_LENGTH)\n",
        "\n",
        "    if tf.is_tensor(text_tensor) and text_tensor.dtype == tf.string:\n",
        "        text = text_tensor.numpy().decode('utf-8')\n",
        "    else:\n",
        "        text = str(text_tensor)\n",
        "\n",
        "    # Optional: Truncate text if it's longer than what model can predict (logit_length)\n",
        "    # logit_length = MAX_MODEL_WIDTH // MODEL_DOWNAMPLE_FACTOR (from global config in Cell 2)\n",
        "    # This ensures len(encoded) <= logit_length\n",
        "    # max_predictable_len = MAX_MODEL_WIDTH // MODEL_DOWNAMPLE_FACTOR\n",
        "    # if len(text) > max_predictable_len:\n",
        "    #     text = text[:max_predictable_len]\n",
        "        # print(f\"Warning: Truncated label from original length to {len(text)} to fit model output.\")\n",
        "\n",
        "    encoded = [char_map_py[char] for char in text if char in char_map_py]\n",
        "\n",
        "    # Pad the encoded label to max_len_py_for_padding (e.g., 68)\n",
        "    padded_label = np.ones(max_len_py_for_padding, dtype=np.int32) * CTC_BLANK_INDEX\n",
        "    padded_label[:len(encoded)] = encoded\n",
        "    return tf.convert_to_tensor(padded_label, dtype=tf.int32), tf.convert_to_tensor(len(encoded), dtype=tf.int32)\n",
        "\n",
        "def preprocess_and_encode_example(example, char_to_num_map_arg, target_height_arg, max_model_width_arg, model_downsample_factor_arg, max_label_len_for_padding_arg):\n",
        "    # char_to_num_map_arg: Python char_to_num dictionary\n",
        "    # target_height_arg: e.g., 64\n",
        "    # max_model_width_arg: e.g., 144 (from Cell 2)\n",
        "    # model_downsample_factor_arg: e.g., 2 (from Cell 2)\n",
        "    # max_label_len_for_padding_arg: e.g., 68 (MAX_LABEL_LENGTH from Cell 4, used for y_true padding)\n",
        "\n",
        "    image_tensor = example['image']\n",
        "    text_label_str = example['text']\n",
        "\n",
        "    image_tensor = tf.squeeze(image_tensor, axis=0)\n",
        "    text_label_str_squeezed = tf.squeeze(text_label_str, axis=0)\n",
        "\n",
        "    image_tensor = tf.cast(image_tensor, tf.float32)\n",
        "\n",
        "    original_shape = tf.shape(image_tensor)\n",
        "    original_height = original_shape[0]\n",
        "    original_width = original_shape[1]\n",
        "\n",
        "    num_channels = tf.shape(image_tensor)[-1]\n",
        "    if image_tensor.shape.rank == 3 and image_tensor.shape[-1] is not None:\n",
        "        num_channels = image_tensor.shape[-1]\n",
        "    else:\n",
        "        num_channels = 3\n",
        "\n",
        "    aspect_ratio = tf.cast(original_width, tf.float32) / tf.cast(original_height, tf.float32)\n",
        "    new_width_float = aspect_ratio * tf.cast(target_height_arg, tf.float32)\n",
        "    new_width = tf.cast(tf.round(new_width_float), tf.int32)\n",
        "    new_width = tf.maximum(new_width, 1)\n",
        "\n",
        "    resized_image = tf.image.resize(image_tensor, [target_height_arg, new_width], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "    current_width_after_resize = tf.shape(resized_image)[1]\n",
        "    if tf.greater(current_width_after_resize, max_model_width_arg):\n",
        "        final_image = tf.image.crop_to_bounding_box(resized_image, 0, 0, target_height_arg, max_model_width_arg)\n",
        "    else:\n",
        "        padding_width = max_model_width_arg - current_width_after_resize\n",
        "        paddings = [[0, 0], [0, padding_width], [0, 0]]\n",
        "        final_image = tf.pad(resized_image, paddings, \"CONSTANT\", constant_values=0.0)\n",
        "\n",
        "    final_image = final_image / 255.0\n",
        "\n",
        "    final_image.set_shape([target_height_arg, max_model_width_arg, num_channels])\n",
        "\n",
        "    py_func_wrapper = lambda text_tensor_for_py_func: encode_text_to_labels(\n",
        "        text_tensor_for_py_func,\n",
        "        char_to_num_map_arg,\n",
        "        max_label_len_for_padding_arg # Pass the padding length for y_true\n",
        "    )\n",
        "\n",
        "    encoded_label, label_length = tf.py_function(\n",
        "        func=py_func_wrapper,\n",
        "        inp=[text_label_str_squeezed],\n",
        "        Tout=[tf.int32, tf.int32]\n",
        "    )\n",
        "    encoded_label.set_shape([max_label_len_for_padding_arg])\n",
        "    label_length.set_shape([])\n",
        "\n",
        "    # input_length for CTC is the model's output sequence length (logit_length)\n",
        "    input_length = tf.convert_to_tensor(max_model_width_arg // model_downsample_factor_arg, dtype=tf.int32)\n",
        "    input_length.set_shape([])\n",
        "\n",
        "    inputs = {\n",
        "        \"image\": final_image,\n",
        "        \"label_length\": label_length,\n",
        "        \"input_length\": input_length\n",
        "    }\n",
        "    return inputs, encoded_label\n",
        "\n",
        "partial_preprocess_encode_fn = partial(\n",
        "    preprocess_and_encode_example,\n",
        "    char_to_num_map_arg=char_to_num, # From Cell 4\n",
        "    target_height_arg=TARGET_PREPROC_HEIGHT, # From Cell 2\n",
        "    max_model_width_arg=MAX_MODEL_WIDTH, # From Cell 2\n",
        "    model_downsample_factor_arg=MODEL_DOWNAMPLE_FACTOR, # From Cell 2\n",
        "    max_label_len_for_padding_arg=MAX_LABEL_LENGTH # From Cell 4 (e.g., 68)\n",
        ")"
      ],
      "id": "vl9p-1igo0IN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4skC5vJTo0IN"
      },
      "source": [
        "## 6. Create TensorFlow Datasets"
      ],
      "id": "4skC5vJTo0IN"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO40f5GVo0IO",
        "outputId": "14342a4e-2322-4600-85ac-fdce7d1c9a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating TensorFlow datasets...\n",
            "TensorFlow datasets (unbatched but processed) created.\n",
            "Train dataset element spec (unbatched): ({'image': TensorSpec(shape=(64, 144, 3), dtype=tf.float32, name=None), 'label_length': TensorSpec(shape=(), dtype=tf.int32, name=None), 'input_length': TensorSpec(shape=(), dtype=tf.int32, name=None)}, TensorSpec(shape=(68,), dtype=tf.int32, name=None))\n",
            "\n",
            "Batched train dataset element spec: ({'image': TensorSpec(shape=(None, 64, 144, 3), dtype=tf.float32, name=None), 'label_length': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'input_length': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 68), dtype=tf.int32, name=None))\n",
            "\n",
            "--- Input Data Sanity Check (First Training Batch BEFORE model.fit) ---\n",
            "Image batch shape: (16, 64, 144, 3)\n",
            "Image batch dtype: <dtype: 'float32'>\n",
            "Image batch min/max pixel values: 0.0000 / 1.0000\n",
            "Image batch mean/std pixel values: 0.7707 / 0.3100\n",
            "Image batch contains NaN: False\n",
            "Image batch contains Inf: False\n",
            "Labels batch shape: (16, 68)\n",
            "Labels batch dtype: <dtype: 'int32'>\n",
            "Sample label (first item, first 15 vals): [52 38 53 34 49 34 44 13  0  0  0  0  0  0  0]\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nCreating TensorFlow datasets...\")\n",
        "\n",
        "tf_train_dataset_unbatched = train_dataset_hf.to_tf_dataset(\n",
        "    columns=['image', 'text'],\n",
        "    shuffle=True,\n",
        "    batch_size=1,\n",
        "    prefetch=tf.data.AUTOTUNE\n",
        ").map(partial_preprocess_encode_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "tf_validation_dataset_unbatched = validation_dataset_hf.to_tf_dataset(\n",
        "    columns=['image', 'text'],\n",
        "    shuffle=False,\n",
        "    batch_size=1,\n",
        "    prefetch=tf.data.AUTOTUNE\n",
        ").map(partial_preprocess_encode_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "tf_test_dataset_unbatched = test_dataset_hf.to_tf_dataset(\n",
        "    columns=['image', 'text'],\n",
        "    shuffle=False,\n",
        "    batch_size=1,\n",
        "    prefetch=tf.data.AUTOTUNE\n",
        ").map(partial_preprocess_encode_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"TensorFlow datasets (unbatched but processed) created.\")\n",
        "print(\"Train dataset element spec (unbatched):\", tf_train_dataset_unbatched.element_spec)\n",
        "\n",
        "train_ds_batched = tf_train_dataset_unbatched.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "validation_ds_batched = tf_validation_dataset_unbatched.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds_batched = tf_test_dataset_unbatched.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"\\nBatched train dataset element spec:\", train_ds_batched.element_spec)\n",
        "\n",
        "print(\"\\n--- Input Data Sanity Check (First Training Batch BEFORE model.fit) ---\")\n",
        "for sample_x_check, sample_y_check in train_ds_batched.take(1):\n",
        "    image_batch_check = sample_x_check['image']\n",
        "    print(\"Image batch shape:\", image_batch_check.shape)\n",
        "    print(\"Image batch dtype:\", image_batch_check.dtype)\n",
        "\n",
        "    min_val = tf.reduce_min(image_batch_check).numpy()\n",
        "    max_val = tf.reduce_max(image_batch_check).numpy()\n",
        "    mean_val = tf.reduce_mean(image_batch_check).numpy()\n",
        "    std_val = tf.math.reduce_std(image_batch_check).numpy()\n",
        "\n",
        "    print(f\"Image batch min/max pixel values: {min_val:.4f} / {max_val:.4f}\")\n",
        "    print(f\"Image batch mean/std pixel values: {mean_val:.4f} / {std_val:.4f}\")\n",
        "\n",
        "    has_nan = tf.reduce_any(tf.math.is_nan(image_batch_check)).numpy()\n",
        "    has_inf = tf.reduce_any(tf.math.is_inf(image_batch_check)).numpy()\n",
        "    print(f\"Image batch contains NaN: {has_nan}\")\n",
        "    print(f\"Image batch contains Inf: {has_inf}\")\n",
        "\n",
        "    print(\"Labels batch shape:\", sample_y_check.shape)\n",
        "    print(\"Labels batch dtype:\", sample_y_check.dtype)\n",
        "    print(\"Sample label (first item, first 15 vals):\", sample_y_check[0,:15].numpy())\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    break"
      ],
      "id": "JO40f5GVo0IO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2xI6NLBo0IO"
      },
      "source": [
        "## 7. Build the CRNN Model"
      ],
      "id": "b2xI6NLBo0IO"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxMxfZkUo0IP",
        "outputId": "b7b40011-6e47-4541-c125-b1c25c8227b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determined model input shape for model build: (64, 144, 3)\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Build the CRNN Model (Adjusted from Debug for Training)\n",
        "\n",
        "def build_crnn_model(input_shape, vocab_size, rnn_units=16): # Renamed function\n",
        "    img_input = layers.Input(shape=input_shape, name=\"image\", dtype=\"float32\")\n",
        "\n",
        "    # Layer 1\n",
        "    x = layers.Conv2D(16, (3, 3), activation=\"relu\",\n",
        "                      kernel_initializer=keras.initializers.VarianceScaling(scale=0.1),\n",
        "                      bias_initializer='zeros',\n",
        "                      padding=\"same\", name=\"conv1\")(img_input)\n",
        "    # Layer 2 (BN was removed in debug, keep it that way or add it back if desired)\n",
        "    # x = layers.BatchNormalization(name=\"bn1\")(x)\n",
        "    # Layer 3\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
        "\n",
        "    new_height = x.shape[1]\n",
        "    new_width = x.shape[2]\n",
        "    num_features = x.shape[3]\n",
        "\n",
        "    if new_height is None or new_width is None or num_features is None:\n",
        "        raise ValueError(f\"Shape component is None after pool1. H:{new_height}, W:{new_width}, F:{num_features}\")\n",
        "\n",
        "    # Layer 4\n",
        "    x = layers.Reshape(target_shape=(new_width, new_height * num_features), name=\"reshape1\")(x)\n",
        "\n",
        "    # Layer 5\n",
        "    x = layers.Dense(16, activation=\"relu\",\n",
        "                     kernel_initializer=keras.initializers.VarianceScaling(scale=0.1),\n",
        "                     bias_initializer='zeros', name=\"dense1\")(x)\n",
        "\n",
        "    # Layer 6\n",
        "    gru_layer = layers.GRU(rnn_units, return_sequences=True,\n",
        "                           kernel_initializer=keras.initializers.VarianceScaling(scale=0.1),\n",
        "                           recurrent_initializer='orthogonal',\n",
        "                           bias_initializer='zeros',\n",
        "                           activation='tanh',\n",
        "                           recurrent_activation='sigmoid',\n",
        "                           name=\"gru1\")\n",
        "    x = layers.Bidirectional(gru_layer, name=\"bigru1\")(x)\n",
        "\n",
        "    # Layer 7\n",
        "    output_logits = layers.Dense(vocab_size, activation=\"linear\",\n",
        "                                 kernel_initializer=keras.initializers.VarianceScaling(scale=0.1),\n",
        "                                 bias_initializer='zeros',\n",
        "                                 name=\"logits\")(x)\n",
        "\n",
        "    # Model for actual training outputs only the final logits\n",
        "    model = keras.Model(\n",
        "        inputs=img_input,\n",
        "        outputs=output_logits, # **** CHANGED THIS LINE ****\n",
        "        name=\"crnn_training_model\" # Optional: give it a distinct name\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# This part is for getting shape info and is fine to keep\n",
        "example_input_dict_spec, _ = train_ds_batched.element_spec # train_ds_batched from Cell 6\n",
        "model_input_shape = example_input_dict_spec['image'].shape[1:]\n",
        "print(f\"Determined model input shape for model build: {model_input_shape}\")\n",
        "\n",
        "# You don't need to build crnn_debug_model here anymore if Cell 9 is for actual training.\n",
        "# crnn_debug_model = build_crnn_model_for_debug(input_shape=model_input_shape, vocab_size=VOCAB_SIZE)\n",
        "# crnn_debug_model.summary()"
      ],
      "id": "dxMxfZkUo0IP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWv3frMgo0IP"
      },
      "source": [
        "## 8. CTC Loss Function"
      ],
      "id": "zWv3frMgo0IP"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hIbHcm8Go0IP"
      },
      "outputs": [],
      "source": [
        "# Cell 8: CTC Loss Function\n",
        "\n",
        "class CTCLossTFNN(keras.losses.Loss):\n",
        "    def __init__(self, name=\"ctc_loss_tfnn\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred_logits):\n",
        "        y_true = tf.cast(y_true, dtype=tf.int32)\n",
        "\n",
        "        y_pred_logits_finite = tf.where(\n",
        "            tf.math.is_finite(y_pred_logits),\n",
        "            y_pred_logits,\n",
        "            tf.zeros_like(y_pred_logits)\n",
        "        )\n",
        "        LOGIT_CLIP_VALUE = 10.0 # Can be adjusted\n",
        "        y_pred_logits_clipped = tf.clip_by_value(y_pred_logits_finite, -LOGIT_CLIP_VALUE, LOGIT_CLIP_VALUE)\n",
        "\n",
        "        logits_time_major = tf.transpose(y_pred_logits_clipped, perm=[1, 0, 2])\n",
        "\n",
        "        label_length_sum = tf.reduce_sum(tf.cast(tf.not_equal(y_true, CTC_BLANK_INDEX), dtype=tf.int32), axis=-1)\n",
        "        label_length = tf.cast(label_length_sum, dtype=tf.int32)\n",
        "\n",
        "        current_batch_size = tf.shape(y_true)[0]\n",
        "        time_steps = tf.shape(logits_time_major)[0]\n",
        "        logit_length = tf.fill(dims=[current_batch_size], value=time_steps)\n",
        "        logit_length = tf.cast(logit_length, dtype=tf.int32)\n",
        "\n",
        "        indices = tf.where(tf.not_equal(y_true, CTC_BLANK_INDEX))\n",
        "        values = tf.gather_nd(y_true, indices)\n",
        "        values = tf.cast(values, dtype=tf.int32)\n",
        "\n",
        "        shape = tf.cast(tf.shape(y_true), tf.int64)\n",
        "        labels_sparse = tf.SparseTensor(indices, values, shape)\n",
        "\n",
        "        # REMOVED/COMMENTED OUT tf.print statements for DTypes and Shapes\n",
        "        # tf.print(\"--- DTypes and Shapes before tf.nn.ctc_loss ---\", summarize=-1)\n",
        "        # tf.print(\"labels_sparse.dtype:\", labels_sparse.dtype, \"shape:\", tf.shape(labels_sparse.dense_shape), summarize=-1)\n",
        "        # tf.print(\"logits_time_major.dtype:\", logits_time_major.dtype, \"shape:\", tf.shape(logits_time_major), summarize=-1)\n",
        "        # tf.print(\"label_length.dtype:\", label_length.dtype, \"shape:\", tf.shape(label_length), summarize=-1)\n",
        "        # tf.print(\"logit_length.dtype:\", logit_length.dtype, \"shape:\", tf.shape(logit_length), summarize=-1)\n",
        "        # tf.print(\"----------------------------------------------\", summarize=-1)\n",
        "\n",
        "        loss = tf.nn.ctc_loss(\n",
        "            labels=labels_sparse,\n",
        "            logits=logits_time_major,\n",
        "            label_length=label_length,\n",
        "            logit_length=logit_length,\n",
        "            logits_time_major=True,\n",
        "            blank_index=CTC_BLANK_INDEX\n",
        "        )\n",
        "\n",
        "        # REMOVED/COMMENTED OUT tf.print for NaN loss check\n",
        "        # tf.cond(tf.reduce_any(tf.math.is_nan(loss)),\n",
        "        #         lambda: tf.print(\"CRITICAL TF.NN.CTC_LOSS: Loss value from tf.nn.ctc_loss IS NaN.\"),\n",
        "        #         lambda: tf.constant(False)\n",
        "        #        )\n",
        "\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "ctc_loss_fn = CTCLossTFNN()"
      ],
      "id": "hIbHcm8Go0IP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-7Vkgf-o0IQ"
      },
      "source": [
        "## 9. Compile and Train the Model"
      ],
      "id": "C-7Vkgf-o0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "OEJu_uLQo0IQ",
        "outputId": "e5035a90-5769-44cf-bb2c-50ba22d3f893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rebuilding the CRNN model for training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"crnn_training_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"crnn_training_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ image (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │           \u001b[38;5;34m448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape1 (\u001b[38;5;33mReshape\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m8,208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bigru1 (\u001b[38;5;33mBidirectional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m3,264\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ logits (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │         \u001b[38;5;34m2,145\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bigru1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,264</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,145</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,065\u001b[0m (54.94 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,065</span> (54.94 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,065\u001b[0m (54.94 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,065</span> (54.94 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled for graph execution (efficient training).\n",
            "\n",
            "Starting model training for 10 epochs (or until EarlyStopping)...\n",
            "Epoch 1/10\n",
            "\u001b[1m  1/473\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:07:42\u001b[0m 39s/step - loss: 267.6516"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3371600166>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# EPOCHS should be defined in your Configuration cell (Cell 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m history = crnn_model.fit(\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mtrain_ds_batched\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_ds_batched\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cell 9: Compile and Train the CRNN Model (For Actual Training)\n",
        "\n",
        "print(\"\\nRebuilding the CRNN model for training...\")\n",
        "# Ensure 'build_crnn_model' (defined in Cell 7 or a similar cell)\n",
        "# is your intended training model architecture.\n",
        "# 'model_input_shape' and 'VOCAB_SIZE' should be available from previous cells.\n",
        "crnn_model = build_crnn_model(input_shape=model_input_shape, vocab_size=VOCAB_SIZE)\n",
        "crnn_model.summary() # Display the model architecture being trained\n",
        "\n",
        "# --- Optimizer Configuration ---\n",
        "# Start with a common learning rate, adjust based on training behavior.\n",
        "# clipvalue can help prevent exploding gradients in RNNs.\n",
        "initial_learning_rate = 1e-3 # A common starting point\n",
        "optimizer = keras.optimizers.Adam(learning_rate=initial_learning_rate, clipvalue=1.0) # Adjust clipvalue if needed\n",
        "\n",
        "# --- Compile the Model ---\n",
        "# run_eagerly should be False (default) for efficient training.\n",
        "crnn_model.compile(optimizer=optimizer, loss=ctc_loss_fn, run_eagerly=False)\n",
        "print(\"Model compiled for graph execution (efficient training).\")\n",
        "\n",
        "# --- Callbacks ---\n",
        "callbacks_list = [\n",
        "    keras.callbacks.TerminateOnNaN(), # Stops training if loss becomes NaN\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,        # Number of epochs with no improvement after which training will be stopped\n",
        "        restore_best_weights=True, # Restores model weights from the epoch with the best val_loss\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,         # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
        "        patience=2,         # Number of epochs with no improvement after which learning rate will be reduced.\n",
        "        min_lr=1e-6,        # Lower bound on the learning rate.\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Optional: ModelCheckpoint to save the best model found during training\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=DRIVE_MODEL_SAVE_PATH, # Ensure DRIVE_MODEL_SAVE_PATH is defined (from Cell 2)\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True, # Only save a model if `val_loss` has improved\n",
        "        save_weights_only=False, # Save the full model\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# --- Start Training ---\n",
        "print(f\"\\nStarting model training for {EPOCHS} epochs (or until EarlyStopping)...\")\n",
        "# EPOCHS should be defined in your Configuration cell (Cell 2)\n",
        "\n",
        "history = crnn_model.fit(\n",
        "    train_ds_batched,\n",
        "    validation_data=validation_ds_batched,\n",
        "    epochs=EPOCHS, # Use the global EPOCHS variable\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1 # Or 2 for less output per epoch\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "# --- Plot Training History (if training ran for at least one epoch) ---\n",
        "if history and history.history:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 1, 1) # If only plotting loss\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model CTC Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"History object not populated, possibly due to training interruption or zero epochs completed.\")\n",
        "\n",
        "# Note: The model saved by ModelCheckpoint (DRIVE_MODEL_SAVE_PATH) is the one with the best val_loss.\n",
        "# The 'crnn_model' instance in memory will have the weights from the last epoch of training\n",
        "# (unless EarlyStopping with restore_best_weights=True updated them).\n",
        "# Cell 10 will save the *current state* of crnn_model after fit completes.\n",
        "# If EarlyStopping restored best weights, then crnn_model and the checkpointed model are the same."
      ],
      "id": "OEJu_uLQo0IQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VgWN4-Ho0IQ"
      },
      "source": [
        "## 10. Save the Trained Model"
      ],
      "id": "3VgWN4-Ho0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMzMoxoqo0IQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Attempting to save model to: {DRIVE_MODEL_SAVE_PATH}\")\n",
        "try:\n",
        "    # Ensure the directory exists\n",
        "    if not os.path.exists(DRIVE_MODEL_SAVE_DIR):\n",
        "        os.makedirs(DRIVE_MODEL_SAVE_DIR)\n",
        "        print(f\"Created directory: {DRIVE_MODEL_SAVE_DIR}\")\n",
        "\n",
        "    crnn_model.save(DRIVE_MODEL_SAVE_PATH)\n",
        "    print(f\"Model saved successfully to {DRIVE_MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # Optional: Verify by listing the file if in Colab and saved to Drive\n",
        "    if GOOGLE_DRIVE_MOUNTED and os.path.exists(DRIVE_MODEL_SAVE_PATH):\n",
        "        print(\"Verifying file in Google Drive (listing using get_ipython().system()):\")\n",
        "        # Use get_ipython().system for cleaner execution of shell commands with variables\n",
        "        path_to_list = str(DRIVE_MODEL_SAVE_PATH) # Ensure it's a plain string\n",
        "        get_ipython().system(f'ls -lh \"{path_to_list}\"')\n",
        "    elif os.path.exists(DRIVE_MODEL_SAVE_PATH):\n",
        "        print(\"Verifying file locally (listing using get_ipython().system()):\")\n",
        "        path_to_list = str(DRIVE_MODEL_SAVE_PATH)\n",
        "        get_ipython().system(f'ls -lh \"{path_to_list}\"')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")"
      ],
      "id": "oMzMoxoqo0IQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo2NQH9bo0IR"
      },
      "source": [
        "## 11. Evaluation and Prediction Example (Using the Trained Model)"
      ],
      "id": "xo2NQH9bo0IR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67dcU_6Fo0IR"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Evaluation and Prediction Example (Using the Trained Model) - With Logit Debugging\n",
        "\n",
        "def ctc_decode_with_tf_nn(y_pred_logits, num_to_char_map):\n",
        "    # y_pred_logits shape: (batch_size, time_steps, num_classes)\n",
        "\n",
        "    logits_time_major = tf.transpose(y_pred_logits, perm=[1, 0, 2]) # (time_steps, batch_size, num_classes)\n",
        "\n",
        "    num_time_steps = tf.shape(logits_time_major)[0] # Scalar tensor, e.g., 72\n",
        "    batch_s = tf.shape(logits_time_major)[1]        # Scalar tensor, e.g., 16\n",
        "\n",
        "    # logit_sequence_length should be a 1D tensor of shape [batch_size]\n",
        "    # where each element is num_time_steps.\n",
        "    logit_sequence_length = tf.fill(dims=[batch_s], value=num_time_steps) # Use list for dims\n",
        "    logit_sequence_length = tf.cast(logit_sequence_length, dtype=tf.int32) # Ensure int32\n",
        "\n",
        "    decoded_list, neg_sum_logits = tf.nn.ctc_greedy_decoder(\n",
        "        inputs=logits_time_major,             # Logits, time-major\n",
        "        sequence_length=logit_sequence_length, # Length of each sequence in the batch\n",
        "        merge_repeated=True                   # Standard for CTC greedy\n",
        "    )\n",
        "\n",
        "    # decoded_list[0] is the SparseTensor representing the decoded paths\n",
        "    sparse_tensor_output = decoded_list[0]\n",
        "\n",
        "    try:\n",
        "        decoded_sequences_num = tf.sparse.to_dense(sparse_tensor_output, default_value=-1).numpy()\n",
        "    except TypeError as e:\n",
        "        print(f\"ERROR: TypeError with tf.nn.ctc_greedy_decoder output to_dense: {e}\")\n",
        "        print(f\"Type of object passed to tf.sparse.to_dense: {type(sparse_tensor_output)}\")\n",
        "        if hasattr(sparse_tensor_output, 'indices') and \\\n",
        "           hasattr(sparse_tensor_output, 'values') and \\\n",
        "           hasattr(sparse_tensor_output, 'dense_shape'):\n",
        "            print(\"Attempting to manually create SparseTensor for to_dense...\")\n",
        "            try:\n",
        "                st_manual = tf.SparseTensor(\n",
        "                    indices=sparse_tensor_output.indices,\n",
        "                    values=sparse_tensor_output.values,\n",
        "                    dense_shape=sparse_tensor_output.dense_shape)\n",
        "                decoded_sequences_num = tf.sparse.to_dense(st_manual, default_value=-1).numpy()\n",
        "            except Exception as e_manual:\n",
        "                print(f\"Manual SparseTensor creation failed: {e_manual}\")\n",
        "                # Return a list of error strings with the correct batch size\n",
        "                error_list = [\"<DECODE_ERROR_MANUAL_ST_FAILED>\"] * batch_s.numpy()\n",
        "                return error_list\n",
        "        else:\n",
        "            # Return a list of error strings with the correct batch size\n",
        "            error_list = [\"<DECODE_ERROR_TYPE_UNKNOWN>\"] * batch_s.numpy()\n",
        "            return error_list\n",
        "\n",
        "    decoded_texts = []\n",
        "    for seq_num in decoded_sequences_num:\n",
        "        text = \"\".join([num_to_char_map.get(num, '') for num in seq_num if num != -1 and num != CTC_BLANK_INDEX])\n",
        "        decoded_texts.append(text)\n",
        "    return decoded_texts\n",
        "\n",
        "\n",
        "print(\"\\nMaking predictions on some test examples...\")\n",
        "\n",
        "num_examples_to_show = 5\n",
        "example_count = 0\n",
        "\n",
        "if BATCH_SIZE > 0 : # Ensure BATCH_SIZE is defined and positive\n",
        "    num_batches_to_take = (num_examples_to_show + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "else:\n",
        "    num_batches_to_take = 1 # Default to taking 1 batch if BATCH_SIZE is invalid\n",
        "\n",
        "for batch_data in test_ds_batched.take(num_batches_to_take): # test_ds_batched from Cell 6\n",
        "    input_dict, true_labels_encoded_batch = batch_data\n",
        "    images_batch = input_dict['image']\n",
        "\n",
        "    preds_logits = crnn_model.predict(images_batch) # crnn_model from Cell 9\n",
        "\n",
        "    # --- Logit Debugging Start ---\n",
        "    print(f\"\\nShape of preds_logits for current batch: {preds_logits.shape}\")\n",
        "    if preds_logits.shape[0] > 0: # Check if batch is not empty\n",
        "        first_image_logits = preds_logits[0]\n",
        "        print(f\"Shape of logits for first image in batch: {first_image_logits.shape}\")\n",
        "        most_likely_indices_per_timestep = np.argmax(first_image_logits, axis=-1)\n",
        "        print(f\"Most likely char indices per time step for first image: {most_likely_indices_per_timestep}\")\n",
        "        # Ensure num_to_char and CTC_BLANK_INDEX are available (defined in Cell 4)\n",
        "        decoded_chars_before_ctc_processing = [num_to_char.get(idx, f\"<UNK_{idx}>\") for idx in most_likely_indices_per_timestep]\n",
        "        print(f\"Chars before CTC processing for first image (joined): '{''.join(decoded_chars_before_ctc_processing)}'\")\n",
        "        print(f\"Chars before CTC processing for first image (space separated): {' '.join(decoded_chars_before_ctc_processing)}\")\n",
        "    # --- Logit Debugging End ---\n",
        "\n",
        "    decoded_preds_greedy = ctc_decode_with_tf_nn(preds_logits, num_to_char)\n",
        "\n",
        "    for i in range(images_batch.shape[0]):\n",
        "        if example_count >= num_examples_to_show:\n",
        "            break\n",
        "\n",
        "        current_true_label_encoded_numpy = true_labels_encoded_batch[i].numpy()\n",
        "        true_text = \"\".join([num_to_char.get(n, '') for n in current_true_label_encoded_numpy if n != -1 and n != CTC_BLANK_INDEX])\n",
        "\n",
        "        print(f\"\\n--- Test Example {example_count + 1} ---\")\n",
        "        print(f\"  True Label        : {true_text}\")\n",
        "        print(f\"  Predicted (Greedy): {decoded_preds_greedy[i]}\") # This is the final output\n",
        "\n",
        "        # Display image\n",
        "        plt.figure(figsize=(10,2)) # Adjust figsize as needed\n",
        "        plt.imshow(images_batch[i].numpy()) # Assuming image is normalized [0,1]\n",
        "        plt.title(f\"True: {true_text}\\nPred (G): {decoded_preds_greedy[i]}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        example_count += 1\n",
        "\n",
        "    if example_count >= num_examples_to_show:\n",
        "        break\n",
        "\n",
        "print(\"\\nPrediction example display complete.\")"
      ],
      "id": "67dcU_6Fo0IR"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}