{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\n","# # 1. Import bibliotek i ustawienia\n","\n","import os\n","import random\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torchvision import transforms\n","from torchvision.datasets import MNIST, EMNIST\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","# Ustawienie losowości dla powtarzalności\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(seed)\n","\n","# Ustawienie urządzenia\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Urządzenie: {device}\")\n","\n","\n","\n","# # 2. Pobranie i przygotowanie danych\n","\n","\n","# Wybór datasetu: 'MNIST' lub 'EMNIST'\n","dataset_name = 'MNIST'  # lub 'EMNIST'\n","# Jeśli EMNIST: wybierz split, np. 'balanced', 'letters', 'digits', itp.\n","emnist_split = 'balanced'\n","\n","# Transformacje: do jednego kanału, normalizacja\n","img_size = 28  # MNIST już 28x28; EMNIST też\n","transform = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","# Ścieżka do katalogu z danymi\n","data_root = './data'\n","\n","if dataset_name == 'MNIST':\n","    # MNIST: train i test\n","    train_full = MNIST(root=data_root, train=True, download=True, transform=transform)\n","    test_dataset = MNIST(root=data_root, train=False, download=True, transform=transform)\n","    classes = [str(i) for i in range(10)]\n","    num_classes = 10\n","    # Jeśli chcemy około 10000 rekordów: weźmy losowy podzbiór z train_full\n","    desired_total = 10000\n","    if len(train_full) >= desired_total:\n","        indices = list(range(len(train_full)))\n","        rnd = torch.Generator().manual_seed(seed)\n","        subset_indices = torch.randperm(len(train_full), generator=rnd)[:desired_total].tolist()\n","        subset_train = Subset(train_full, subset_indices)\n","    else:\n","        print(f\"Train dataset MNIST ma mniej niż {desired_total} obrazów: używamy pełnego zbioru train (rozmiar {len(train_full)})\")\n","        subset_train = train_full\n","    # Podział subset_train na train/val/test proporcje 80/10/10 w ramach subsetu\n","    num_samples = len(subset_train)\n","    train_size = int(0.8 * num_samples)\n","    val_size = int(0.1 * num_samples)\n","    test_size = num_samples - train_size - val_size\n","    train_dataset, val_dataset, extra_test = random_split(subset_train, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n","    # Można połączyć extra_test z test_dataset lub traktować oddzielnie. Tutaj traktujemy oddzielnie: finalny test to test_dataset z MNIST pełnym.\n","    # Jeśli chcemy testować tylko na extra_test, można to zmienić.\n","    use_full_test = True\n","elif dataset_name == 'EMNIST':\n","    # EMNIST: train i test w podziale split\n","    # Wymaga konwersji obrazów: EMNIST zapisane w pionie? torchvision automatycznie rotuje?\n","    train_full = EMNIST(root=data_root, split=emnist_split, train=True, download=True, transform=transform)\n","    test_dataset = EMNIST(root=data_root, split=emnist_split, train=False, download=True, transform=transform)\n","    classes = train_full.classes if hasattr(train_full, 'classes') else None\n","    # Jeśli brak classes, można generować etykiety jako indeksy\n","    num_classes = len(train_full.classes) if hasattr(train_full, 'classes') else len(set(train_full.targets.tolist()))\n","    # Podzbiór ~10000 z train_full\n","    desired_total = 10000\n","    if len(train_full) >= desired_total:\n","        indices = list(range(len(train_full)))\n","        rnd = torch.Generator().manual_seed(seed)\n","        subset_indices = torch.randperm(len(train_full), generator=rnd)[:desired_total].tolist()\n","        subset_train = Subset(train_full, subset_indices)\n","    else:\n","        print(f\"Train dataset EMNIST ma mniej niż {desired_total} obrazów: używamy pełnego zbioru train (rozmiar {len(train_full)})\")\n","        subset_train = train_full\n","    num_samples = len(subset_train)\n","    train_size = int(0.8 * num_samples)\n","    val_size = int(0.1 * num_samples)\n","    test_size = num_samples - train_size - val_size\n","    train_dataset, val_dataset, extra_test = random_split(subset_train, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n","    use_full_test = True\n","else:\n","    raise ValueError(\"Nieznany dataset_name. Wybierz 'MNIST' lub 'EMNIST'.\")\n","\n","print(f\"Rozkład: Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n","if use_full_test:\n","    print(f\"Dostępny oddzielny zbiór testowy: {len(test_dataset)} obrazów\")\n","else:\n","    print(f\"Test (z subsetu): {len(extra_test)} obrazów\")\n","\n","# DataLoaders\n","dataloader_params = {'batch_size': 64, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if torch.cuda.is_available() else {'batch_size': 64, 'shuffle': True}\n","train_loader = DataLoader(train_dataset, **dataloader_params)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4) if torch.cuda.is_available() else DataLoader(val_dataset, batch_size=64, shuffle=False)\n","if use_full_test:\n","    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4) if torch.cuda.is_available() else DataLoader(test_dataset, batch_size=64, shuffle=False)\n","else:\n","    test_loader = DataLoader(extra_test, batch_size=64, shuffle=False, num_workers=4) if torch.cuda.is_available() else DataLoader(extra_test, batch_size=64, shuffle=False)\n","\n","\n","\n","# # 3. Definicja własnej architektury CNN\n","\n","class HandwritingCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(HandwritingCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 28x28 -> 14x14\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 14x14 -> 7x7\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),  # 7x7 -> 4x4\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(128 * 4 * 4, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","model = HandwritingCNN(num_classes=num_classes).to(device)\n","print(model)\n","\n","\n","# %%\n","# # 4. Funkcje treningu i walidacji\n","\n","# %%\n","def train_one_epoch(model, dataloader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for inputs, labels in tqdm(dataloader, desc=\"Trenowanie\", leave=False):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","    return epoch_loss, epoch_acc\n","\n","\n","def validate_one_epoch(model, dataloader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Walidacja\", leave=False):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","    return epoch_loss, epoch_acc\n","\n","\n","# %%\n","# # 5. Trenowanie modelu\n","\n","# %%\n","epochs = 30\n","learning_rate = 1e-3\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","best_val_acc = 0.0\n","save_path = \"handwriting_cnn_best.pth\"\n","history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n","\n","for epoch in range(1, epochs+1):\n","    print(f\"Epoka {epoch}/{epochs}\")\n","    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n","\n","    history['train_loss'].append(train_loss)\n","    history['train_acc'].append(train_acc)\n","    history['val_loss'].append(val_loss)\n","    history['val_acc'].append(val_acc)\n","\n","    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n","    print(f\"  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"  Zapisano najlepszy model, Val Acc: {best_val_acc:.4f}\")\n","\n","\n","\n","# # 6. Ocena na zbiorze testowym\n","\n","# Wczytanie najlepszego modelu\n","model.load_state_dict(torch.load(save_path, map_location=device))\n","\n","test_loss, test_acc = validate_one_epoch(model, test_loader, criterion, device)\n","print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLne25gVCmBC","executionInfo":{"status":"ok","timestamp":1749586070579,"user_tz":-120,"elapsed":78518,"user":{"displayName":"Dominik Świerczyński","userId":"15797185136993680070"}},"outputId":"322e3821-abd8-404a-e9b6-5158b10a3a0f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Urządzenie: cuda\n","Rozkład: Train: 8000, Val: 1000\n","Dostępny oddzielny zbiór testowy: 10000 obrazów\n","HandwritingCNN(\n","  (features): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=2048, out_features=256, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=256, out_features=10, bias=True)\n","  )\n",")\n","Epoka 1/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.8920, Train Acc: 0.7007\n","  Val   Loss: 0.1784, Val   Acc: 0.9460\n","  Zapisano najlepszy model, Val Acc: 0.9460\n","Epoka 2/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.2082, Train Acc: 0.9357\n","  Val   Loss: 0.1176, Val   Acc: 0.9660\n","  Zapisano najlepszy model, Val Acc: 0.9660\n","Epoka 3/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.1479, Train Acc: 0.9536\n","  Val   Loss: 0.0848, Val   Acc: 0.9750\n","  Zapisano najlepszy model, Val Acc: 0.9750\n","Epoka 4/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.1061, Train Acc: 0.9686\n","  Val   Loss: 0.0713, Val   Acc: 0.9790\n","  Zapisano najlepszy model, Val Acc: 0.9790\n","Epoka 5/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0936, Train Acc: 0.9709\n","  Val   Loss: 0.0599, Val   Acc: 0.9810\n","  Zapisano najlepszy model, Val Acc: 0.9810\n","Epoka 6/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0793, Train Acc: 0.9759\n","  Val   Loss: 0.0655, Val   Acc: 0.9780\n","Epoka 7/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0669, Train Acc: 0.9769\n","  Val   Loss: 0.0655, Val   Acc: 0.9780\n","Epoka 8/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0652, Train Acc: 0.9791\n","  Val   Loss: 0.0783, Val   Acc: 0.9740\n","Epoka 9/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0640, Train Acc: 0.9799\n","  Val   Loss: 0.0672, Val   Acc: 0.9810\n","Epoka 10/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0488, Train Acc: 0.9828\n","  Val   Loss: 0.0633, Val   Acc: 0.9800\n","Epoka 11/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0455, Train Acc: 0.9850\n","  Val   Loss: 0.0576, Val   Acc: 0.9810\n","Epoka 12/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0461, Train Acc: 0.9849\n","  Val   Loss: 0.0639, Val   Acc: 0.9850\n","  Zapisano najlepszy model, Val Acc: 0.9850\n","Epoka 13/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0351, Train Acc: 0.9885\n","  Val   Loss: 0.0566, Val   Acc: 0.9820\n","Epoka 14/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0404, Train Acc: 0.9874\n","  Val   Loss: 0.0610, Val   Acc: 0.9850\n","Epoka 15/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0362, Train Acc: 0.9885\n","  Val   Loss: 0.0548, Val   Acc: 0.9860\n","  Zapisano najlepszy model, Val Acc: 0.9860\n","Epoka 16/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0320, Train Acc: 0.9895\n","  Val   Loss: 0.0606, Val   Acc: 0.9820\n","Epoka 17/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0329, Train Acc: 0.9890\n","  Val   Loss: 0.0596, Val   Acc: 0.9830\n","Epoka 18/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0300, Train Acc: 0.9904\n","  Val   Loss: 0.0763, Val   Acc: 0.9800\n","Epoka 19/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0292, Train Acc: 0.9905\n","  Val   Loss: 0.0597, Val   Acc: 0.9850\n","Epoka 20/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0242, Train Acc: 0.9925\n","  Val   Loss: 0.0577, Val   Acc: 0.9860\n","Epoka 21/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0249, Train Acc: 0.9916\n","  Val   Loss: 0.0881, Val   Acc: 0.9750\n","Epoka 22/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0232, Train Acc: 0.9912\n","  Val   Loss: 0.0537, Val   Acc: 0.9880\n","  Zapisano najlepszy model, Val Acc: 0.9880\n","Epoka 23/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0314, Train Acc: 0.9905\n","  Val   Loss: 0.0557, Val   Acc: 0.9840\n","Epoka 24/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0171, Train Acc: 0.9936\n","  Val   Loss: 0.0617, Val   Acc: 0.9860\n","Epoka 25/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0220, Train Acc: 0.9925\n","  Val   Loss: 0.0545, Val   Acc: 0.9850\n","Epoka 26/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0232, Train Acc: 0.9919\n","  Val   Loss: 0.0693, Val   Acc: 0.9810\n","Epoka 27/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0186, Train Acc: 0.9938\n","  Val   Loss: 0.0643, Val   Acc: 0.9860\n","Epoka 28/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0161, Train Acc: 0.9936\n","  Val   Loss: 0.0630, Val   Acc: 0.9880\n","Epoka 29/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0279, Train Acc: 0.9921\n","  Val   Loss: 0.0472, Val   Acc: 0.9870\n","Epoka 30/30\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Loss: 0.0215, Train Acc: 0.9926\n","  Val   Loss: 0.0509, Val   Acc: 0.9880\n"]},{"output_type":"stream","name":"stderr","text":["                                                            "]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.0438, Test Acc: 0.9876\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"code","source":["# %%\n","# # 7. Przykładowe przewidywanie na nowych obrazach\n","\n","# %%\n","def inference_image(model, image_path, transform, classes, device):\n","    \"\"\"\n","    Wykonuje predykcję na pojedynczym obrazie.\n","\n","    Args:\n","        model (torch.nn.Module): Wytrenowany model.\n","        image_path (str): Ścieżka do pliku obrazu.\n","        transform (torchvision.transforms.Compose): Transformacje do zastosowania na obrazie.\n","        classes (list): Lista nazw klas.\n","        device (torch.device): Urządzenie (CPU/GPU).\n","\n","    Returns:\n","        tuple: Przewidziana etykieta i poziom pewności.\n","    \"\"\"\n","    # Wczytanie i przygotowanie obrazu\n","    img = Image.open(image_path).convert('L') # Konwersja na grayscale\n","\n","    # Wyświetlenie obrazu\n","    plt.figure(figsize=(3,3))\n","    plt.imshow(img, cmap='gray')\n","    plt.axis('off')\n","    plt.title(\"Input image\")\n","    plt.show()\n","\n","    # Transformacja i dodanie wymiaru batcha\n","    tensor = transform(img).unsqueeze(0).to(device)\n","\n","    model.eval() # Ustawienie modelu w tryb ewaluacji\n","    with torch.no_grad():\n","        outputs = model(tensor)\n","        probs = F.softmax(outputs, dim=1) # Obliczenie prawdopodobieństw\n","        conf, pred_idx = torch.max(probs, dim=1) # Najwyższe prawdopodobieństwo i indeks klasy\n","\n","    label = classes[pred_idx.item()]\n","    confidence = conf.item()\n","\n","    print(f\"Predykcja: {label}\")\n","\n","\n","    return label, confidence\n","\n","\n","try:\n","\n","\n","    image_path = \"3.png\" # <--- ZMIEŃ TO NA SWOJĄ ŚCIEŻKĘ DO OBRAZU\n","\n","    # Wczytaj architekturę i wagi\n","    model_inf = HandwritingCNN(num_classes=num_classes).to(device)\n","    model_inf.load_state_dict(torch.load(save_path, map_location=device))\n","\n","    # Wykonaj inferencję\n","    label, conf = inference_image(model_inf, image_path, transform, classes, device)\n","\n","except FileNotFoundError:\n","    print(f\"Błąd: Nie znaleziono pliku obrazu pod ścieżką '{image_path}'.\")\n","    print(\"Upewnij się, że ścieżka do obrazu jest poprawna i plik istnieje.\")\n","except Exception as e:\n","    print(f\"Wystąpił błąd podczas inferencji: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"voobvQgHygTZ","executionInfo":{"status":"ok","timestamp":1749586107725,"user_tz":-120,"elapsed":174,"user":{"displayName":"Dominik Świerczyński","userId":"15797185136993680070"}},"outputId":"b66133a1-ed87-4504-d352-b1651b82a94c"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 300x300 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Predykcja: 1\n","Pewność: 36.32%\n"]}]}]}